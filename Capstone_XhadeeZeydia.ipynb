{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7d1ec87-9fbf-4b73-8ad4-e95257b2a311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **PROJET DATA ENGINEERING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4b9b1a4-8e35-4362-90e8-68cec4c89a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ARCHITECTURE DU PROJET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2624bd49-b842-47e9-972b-bbd3df1c39b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"workspace\"\n",
    "SCHEMA  = \"xhadeezeydia\"\n",
    "VOLUME  = \"capstoneipsl\"\n",
    "\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n",
    "\n",
    "VOLUME_ROOT = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n",
    "PROJECT_ROOT = f\"{VOLUME_ROOT}/ecommerce_project\"\n",
    "\n",
    "DIRECTORIES = [\n",
    "\n",
    "    # BRONZE\n",
    "    \"data/bronze/main\",\n",
    "    \"data/bronze/enrich\",\n",
    "\n",
    "    # SILVER\n",
    "    \"data/silver/main_clean\",\n",
    "    \"data/silver/enrich_clean\",\n",
    "    \"data/silver/joined\",\n",
    "\n",
    "    # GOLD\n",
    "    \"data/gold/marts\",\n",
    "    \"data/gold/aggregates\",\n",
    "    \"data/gold/exports\",\n",
    "\n",
    "    # CODE\n",
    "    \"src/ingestion\",\n",
    "    \"src/transforms\",\n",
    "    \"src/quality\",\n",
    "    \"src/utils\",\n",
    "\n",
    "    # ORCHESTRATION\n",
    "    \"notebooks\",\n",
    "    \"configs\",\n",
    "\n",
    "    # REPORTS\n",
    "    \"reports/data_quality\",\n",
    "    \"reports/benchmarks\"\n",
    "]\n",
    "\n",
    "\n",
    "for d in DIRECTORIES:\n",
    "    path = f\"{PROJECT_ROOT}/{d}\"\n",
    "    dbutils.fs.mkdirs(path)\n",
    "    print(f\"‚úì Created: {path}\")\n",
    "\n",
    "\n",
    "dbutils.fs.ls(f\"{PROJECT_ROOT}/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2dc1b36-b585-45e3-85e4-e3f0fa6c6d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## L'Ingestion et l'Amplification(BRONZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5448c439-16e2-44c1-9867-bd0ee44ac218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Chargement et Fusion (Source 1)\n",
    "df_raw = spark.read.csv(f\"{PROJECT_ROOT}/data/bronze/main/*.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2. Amplification Massive \n",
    "df_amplified = df_raw\n",
    "for _ in range(14):\n",
    "    df_amplified = df_amplified.unionAll(df_raw)\n",
    "\n",
    "# 3. √âcriture en PARQUET\n",
    "bronze_main_path = f\"{PROJECT_ROOT}/data/bronze/main/full_data.parquet\"\n",
    "df_amplified.write.mode(\"overwrite\").parquet(bronze_main_path)\n",
    "\n",
    "# 4. Source 2 (Enrichissement)\n",
    "enrich_data = [(\"electronics\", \"High-Tech\", 0.20), (\"appliances\", \"Home\", 0.15), (\"computers\", \"IT\", 0.18)]\n",
    "df_enrich = spark.createDataFrame(enrich_data, [\"category_code_prefix\", \"category_department\", \"margin_rate\"])\n",
    "df_enrich.write.mode(\"overwrite\").parquet(f\"{PROJECT_ROOT}/data/bronze/enrich/static_ref.parquet\")\n",
    "\n",
    "# 5. Validation de la taille\n",
    "size_gb = sum(f.size for f in dbutils.fs.ls(bronze_main_path) if f.name.endswith(\".parquet\")) / (1024**3)\n",
    "print(f\"‚úÖ BRONZE VALIDE : {size_gb:.2f} GB | Format: Parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceddddf0-fb39-450d-9633-753115a46ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# On passe √† 20 fois la base (1 initiale + 19 unions)\n",
    "df_amplified = df_raw\n",
    "for _ in range(19): \n",
    "    df_amplified = df_amplified.unionAll(df_raw)\n",
    "\n",
    "# On r√©-√©crit par-dessus\n",
    "df_amplified.write.mode(\"overwrite\").parquet(bronze_main_path)\n",
    "\n",
    "# On re-v√©rifie\n",
    "size_gb = sum(f.size for f in dbutils.fs.ls(bronze_main_path) if f.name.endswith(\".parquet\")) / (1024**3)\n",
    "print(f\"üöÄ NOUVELLE TAILLE BRONZE : {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066b0431-9ac1-419a-840c-23da807f60f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_amplified.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1008ba7a-5704-4c13-8798-4e7c061dccd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Zone SILVER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ad82f37-a2b9-4eef-9ee0-13541e9f8a8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Nettoyage M√©tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fa2dcf3-38c2-4563-a8a3-3263e06cfa5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# --- OPTIMISATION 1 : Column Pruning \n",
    "# On liste uniquement les colonnes utiles pour les analyses Gold et les checks qualit√©\n",
    "needed_columns = [\n",
    "    \"event_time\", \"event_type\", \"product_id\", \"category_id\", \n",
    "    \"category_code\", \"brand\", \"price\", \"user_id\", \"user_session\"\n",
    "]\n",
    "\n",
    "# Chargement optimis√©\n",
    "df_bronze = spark.read.parquet(f\"{PROJECT_ROOT}/data/bronze/main/full_data.parquet\") \\\n",
    "                 .select(*needed_columns)\n",
    "\n",
    "# Filtrage m√©tier\n",
    "df_cleaned = df_bronze.filter(\n",
    "    (col(\"user_id\").isNotNull()) & \n",
    "    (col(\"product_id\").isNotNull()) & \n",
    "    (col(\"price\") > 0)\n",
    ")\n",
    "\n",
    "# --- OPTIMISATION 2 : Re-partitionnement \n",
    "df_cleaned = df_cleaned.repartition(col(\"event_type\")) \n",
    "\n",
    "print(f\"‚úÖ Nettoyage m√©tier termin√©.\")\n",
    "print(f\"Lignes restantes : {df_cleaned.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0d4486c-02bb-473f-b408-5da87ce4d458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Standardisation (Formatage Propre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f792f4fe-456d-44fd-903f-62251c87d6de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, trim, col\n",
    "\n",
    "# Standardisation des textes et types\n",
    "df_silver = df_cleaned \\\n",
    "    .withColumn(\"event_type\", lower(trim(col(\"event_type\")))) \\\n",
    "    .withColumn(\"category_code\", lower(trim(col(\"category_code\")))) \\\n",
    "    .withColumn(\"brand\", lower(trim(col(\"brand\")))) \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"double\"))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Standardisation termin√©e. L'optimisation est g√©r√©e par le moteur Photon/Serverless.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd5c9a2e-9e03-4edb-9485-113dac503ebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Les 8 Checks Qualit√© + Sauvegarde Optimis√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6531c4b9-5f10-494a-80c2-d1fe6dc09898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, when, countDistinct, col\n",
    "\n",
    "total_rows = df_silver.count()\n",
    "\n",
    "# 1. CALCUL DES 8 INDICATEURS\n",
    "quality_metrics = df_silver.select(\n",
    "    ((count(when(col(\"user_id\").isNotNull(), True)) / total_rows) * 100).alias(\"chk_1_user_complete\"),\n",
    "    ((count(when(col(\"price\") > 0, True)) / total_rows) * 100).alias(\"chk_2_price_pos\"),\n",
    "    ((count(when(col(\"event_type\").isNotNull(), True)) / total_rows) * 100).alias(\"chk_3_evt_present\"),\n",
    "    ((count(when(col(\"event_time\").isNotNull(), True)) / total_rows) * 100).alias(\"chk_4_date_valid\"),\n",
    "    ((count(when(col(\"brand\").isNotNull(), True)) / total_rows) * 100).alias(\"chk_5_brand_filled\"),\n",
    "    ((count(when(col(\"category_code\").isNotNull(), True)) / total_rows) * 100).alias(\"chk_6_cat_filled\"),\n",
    "    ((countDistinct(\"user_session\") / total_rows) * 100).alias(\"chk_7_unique_sessions\"),\n",
    "    ((count(when(col(\"user_session\").rlike(\"^[0-9a-fA-F-]+\"), True)) / total_rows) * 100).alias(\"chk_8_session_format\")\n",
    ")\n",
    "\n",
    "print(\"üìä RAPPORT DE QUALIT√â (SILVER) :\")\n",
    "quality_metrics.show()\n",
    "\n",
    "# 2. SAUVEGARDE DU RAPPORT \n",
    "quality_metrics.write.mode(\"overwrite\").parquet(f\"{PROJECT_ROOT}/reports/data_quality/silver_report.parquet\")\n",
    "\n",
    "#  3. SAUVEGARDE SILVER OPTIMIS√âE (Partitionnement)\n",
    "# OPTIMISATION 3 : Partitionnement par 'event_type'\n",
    "# Cela permet d'acc√©l√©rer les analyses Gold qui filtrent par type d'action (view, cart, purchase)\n",
    "silver_path = f\"{PROJECT_ROOT}/data/silver/main_clean.parquet\"\n",
    "\n",
    "df_silver.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"event_type\") \\\n",
    "    .parquet(silver_path)\n",
    "\n",
    "print(f\"‚úÖ Donn√©es Silver sauvegard√©es en Parquet avec partitionnement dans : {silver_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "113ab253-37cb-4610-9478-6bc0bcc3e450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Zone GOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9faddc1-1e11-4dbc-9126-68d3b0130660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Enrichissement avec Broadcast Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddc2a918-c468-4692-be68-f861b1ce80d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast, split, col, when\n",
    "\n",
    "# 1. Chargement des sources\n",
    "df_silver = spark.read.parquet(f\"{PROJECT_ROOT}/data/silver/main_clean.parquet\")\n",
    "df_enrich = spark.read.parquet(f\"{PROJECT_ROOT}/data/bronze/enrich/static_ref.parquet\")\n",
    "\n",
    "# 2. Pr√©paration de la jointure (Correction de la regex avec r\"\\.\")\n",
    "# On extrait le premier mot du category_code (ex: \"electronics.smartphone\" -> \"electronics\")\n",
    "df_silver_prep = df_silver.withColumn(\"cat_prefix\", split(col(\"category_code\"), r\"\\.\").getItem(0))\n",
    "\n",
    "# --- OPTIMISATION 3 : Broadcast Join ---\n",
    "# On utilise r\"broadcast()\" pour envoyer la petite table de r√©f√©rence sur tous les n≈ìuds.\n",
    "# Cela √©vite un Shuffle massif des 9.30 GB de donn√©es Silver.\n",
    "df_gold_base = df_silver_prep.join(\n",
    "    broadcast(df_enrich), \n",
    "    df_silver_prep.cat_prefix == df_enrich.category_code_prefix, \n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# 3. Calcul de la Marge Estim√©e (Indicateur m√©tier bas√© sur l'enrichissement)\n",
    "# Si pas de correspondance, on applique une marge par d√©faut de 10%\n",
    "df_gold_base = df_gold_base.withColumn(\n",
    "    \"estimated_margin\", \n",
    "    when(col(\"margin_rate\").isNotNull(), col(\"price\") * col(\"margin_rate\"))\n",
    "    .otherwise(col(\"price\") * 0.10)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Jointure Broadcast r√©ussie avec enrichissement et marge calcul√©e.\")\n",
    "display(df_gold_base.limit(5)) # Pour v√©rifier visuellement les nouvelles colonnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a389fc6f-c0a6-4b73-aa5f-12d9d5aa3e7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### G√©n√©ration des  Outputs Gold (Analytics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8b90ad-e8a3-4398-a776-8fe5a4091c1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, sum, count, col, round\n",
    "\n",
    "# 1. Enrichissement Brand Perf : Ajout du Panier Moyen (AOV) et % de Marge\n",
    "gold_brand_perf = df_gold_base.filter(col(\"event_type\") == \"purchase\") \\\n",
    "    .groupBy(\"brand\") \\\n",
    "    .agg(\n",
    "        sum(\"price\").alias(\"total_revenue\"),\n",
    "        sum(\"estimated_margin\").alias(\"total_margin\"),\n",
    "        count(\"product_id\").alias(\"sales_count\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_order_value\", round(col(\"total_revenue\") / col(\"sales_count\"), 2)) \\\n",
    "    .withColumn(\"margin_percentage\", round((col(\"total_margin\") / col(\"total_revenue\")) * 100, 2)) \\\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "# 2. Enrichissement Dept Stats : Ajout du poids relatif (%) du d√©partement\n",
    "total_global_revenue = df_gold_base.filter(col(\"event_type\") == \"purchase\").agg(sum(\"price\")).collect()[0][0]\n",
    "\n",
    "gold_dept_stats = df_gold_base.filter(col(\"event_type\") == \"purchase\") \\\n",
    "    .groupBy(\"category_department\") \\\n",
    "    .agg(\n",
    "        count(\"event_type\").alias(\"total_sales\"),\n",
    "        sum(\"price\").alias(\"dept_revenue\")\n",
    "    ) \\\n",
    "    .withColumn(\"revenue_contribution_pct\", round((col(\"dept_revenue\") / total_global_revenue) * 100, 2)) \\\n",
    "    .dropna()\n",
    "\n",
    "# 3. Enrichissement Funnel : Pivot pour faciliter l'export Postgres\n",
    "# On transforme les lignes (view, cart, purchase) en colonnes pour calculer un taux de conversion\n",
    "gold_conversion_pivot = df_gold_base.groupBy(\"brand\") \\\n",
    "    .pivot(\"event_type\") \\\n",
    "    .agg(count(\"user_session\")) \\\n",
    "    .fillna(0) \\\n",
    "    .withColumn(\"conversion_rate_pct\", \n",
    "                round((col(\"purchase\") / col(\"view\")) * 100, 2))\n",
    "\n",
    "# --- SAUVEGARDE DES OUTPUTS ---\n",
    "gold_path = f\"{PROJECT_ROOT}/data/gold\"\n",
    "\n",
    "gold_brand_perf.write.mode(\"overwrite\").parquet(f\"{gold_path}/brand_performance.parquet\")\n",
    "gold_dept_stats.write.mode(\"overwrite\").parquet(f\"{gold_path}/department_stats.parquet\")\n",
    "gold_conversion_pivot.write.mode(\"overwrite\").parquet(f\"{gold_path}/conversion_funnel.parquet\")\n",
    "\n",
    "print(f\"‚úÖ Les  tables Gold ENRICHIES ont √©t√© g√©n√©r√©es dans : {gold_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc285bf-32b9-4b76-b70f-414356e244c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# V√©rification du contenu du dossier Gold\n",
    "display(dbutils.fs.ls(f\"{PROJECT_ROOT}/data/gold\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc5c6ce2-64a2-4ae2-8248-d22ceee1e5e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Performance : Benchmark \"Avant vs Apr√®s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "090c7438-dba3-4e4b-a0d6-c549eb14bc2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import col, lit, round, when, concat\n",
    "\n",
    "# 1. Mesure brute (Bronze)\n",
    "start_csv = time.time()\n",
    "spark.read.csv(f\"{PROJECT_ROOT}/data/bronze/main/*.csv\", header=True).count()\n",
    "duration_csv = float(time.time() - start_csv)\n",
    "\n",
    "# 2. Mesure optimis√©e (Silver)\n",
    "start_pq = time.time()\n",
    "spark.read.parquet(f\"{PROJECT_ROOT}/data/silver/main_clean.parquet\").count()\n",
    "duration_pq = float(time.time() - start_pq)\n",
    "\n",
    "# 3. Mesure stockage\n",
    "def get_size(path):\n",
    "    try:\n",
    "        return float(sum(f.size for f in dbutils.fs.ls(path) if not f.name.startswith(\"_\")) / (1024**3))\n",
    "    except: return 0.1\n",
    "\n",
    "s_csv = get_size(f\"{PROJECT_ROOT}/data/bronze/main\")\n",
    "s_pq = get_size(f\"{PROJECT_ROOT}/data/silver/main_clean.parquet\")\n",
    "\n",
    "# 4. Cr√©ation du DataFrame de base\n",
    "raw_data = [\n",
    "    (\"Temps de lecture (sec)\", duration_csv, duration_pq),\n",
    "    (\"Espace disque (GB)\", s_csv, s_pq)\n",
    "]\n",
    "\n",
    "df_bench = spark.createDataFrame(raw_data, [\"Metrique\", \"Brut_CSV\", \"Optimise_Parquet\"])\n",
    "\n",
    "# 5. Calcul des gains via Spark (Safe pour Serverless)\n",
    "df_final = df_bench.withColumn(\n",
    "    \"Gain\",\n",
    "    when(col(\"Metrique\").contains(\"Temps\"), \n",
    "         concat(round(col(\"Brut_CSV\") / col(\"Optimise_Parquet\"), 1), lit(\"x plus rapide\")))\n",
    "    .otherwise(\n",
    "         concat(round((1 - (col(\"Optimise_Parquet\") / col(\"Brut_CSV\"))) * 100, 1), lit(\"% de reduction\")))\n",
    ")\n",
    "\n",
    "print(\"üöÄ R√âSULTATS DU BENCHMARK :\")\n",
    "df_final.show(truncate=False)\n",
    "\n",
    "# Sauvegarde finale du rapport\n",
    "df_final.write.mode(\"overwrite\").parquet(f\"{PROJECT_ROOT}/reports/benchmarks/performance_final.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "752424e6-e424-4540-a450-1a5e5335fff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Export Postgres & Requ√™tes SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b4c9b71-aff6-490d-a4ef-4a1b9edc4820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Pr√©paration de la table Gold pour l'export SQL\n",
    "# On choisit la table de performance des marques\n",
    "df_gold_brand = spark.read.parquet(f\"{PROJECT_ROOT}/data/gold/brand_performance.parquet\")\n",
    "\n",
    "# 2. Export vers une table SQL locale (Databricks SQL / Hive Metastore)\n",
    "database_name = \"ecommerce_db\"\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "\n",
    "df_gold_brand.write.mode(\"overwrite\").saveAsTable(f\"{database_name}.brand_performance\")\n",
    "\n",
    "print(f\"‚úÖ Export vers la base SQL '{database_name}' termin√©.\")\n",
    "\n",
    "# 3. Requ√™tes SQL (Contrainte obligatoire : 8 checks ou analyses via SQL)\n",
    "print(\"üìä EX√âCUTION DES REQU√äTES SQL ANALYTIQUES :\")\n",
    "\n",
    "# Requ√™te : Top 5 des marques les plus rentables\n",
    "query_result = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        brand, \n",
    "        round(total_revenue, 2) as CA, \n",
    "        round(total_margin, 2) as Marge,\n",
    "        margin_percentage as Rentabilite\n",
    "    FROM {database_name}.brand_performance\n",
    "    WHERE brand IS NOT NULL\n",
    "    ORDER BY total_margin DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "query_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19af8f4c-2277-47c0-a117-3c76ca3acc55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\" G√âN√âRATION DU RAPPORT ANALYTIQUE SQL  :\")\n",
    "\n",
    "# On d√©finit les analyses avec filtres pour exclure les valeurs manquantes dans les tops\n",
    "queries = {\n",
    "    \"1. Top 5 Marques (CA)\": f\"\"\"\n",
    "        SELECT brand, round(total_revenue, 2) as CA \n",
    "        FROM {database_name}.brand_performance \n",
    "        WHERE brand IS NOT NULL \n",
    "        ORDER BY total_revenue DESC LIMIT 5\"\"\",\n",
    "    \n",
    "    \"2. Top 5 Marques (Marge)\": f\"\"\"\n",
    "        SELECT brand, round(total_margin, 2) as Marge \n",
    "        FROM {database_name}.brand_performance \n",
    "        WHERE brand IS NOT NULL \n",
    "        ORDER BY total_margin DESC LIMIT 5\"\"\",\n",
    "    \n",
    "    \"3. Top 5 Rentabilit√© (%)\": f\"\"\"\n",
    "        SELECT brand, margin_percentage \n",
    "        FROM {database_name}.brand_performance \n",
    "        WHERE brand IS NOT NULL AND total_revenue > 1000 \n",
    "        ORDER BY margin_percentage DESC LIMIT 5\"\"\",\n",
    "    \n",
    "    \"4. Volume de ventes total\": f\"SELECT sum(sales_count) as total_ventes FROM {database_name}.brand_performance\",\n",
    "    \n",
    "    \"5. Panier moyen global\": f\"SELECT round(avg(avg_order_value), 2) as panier_moyen FROM {database_name}.brand_performance\",\n",
    "    \n",
    "    \"6. Nombre de marques identifi√©es\": f\"SELECT count(distinct brand) as nb_marques FROM {database_name}.brand_performance WHERE brand IS NOT NULL\",\n",
    "    \n",
    "    \"7. CA Moyen par marque\": f\"SELECT round(avg(total_revenue), 2) FROM {database_name}.brand_performance WHERE brand IS NOT NULL\",\n",
    "    \n",
    "    \"8. Audit des donn√©es (Marques manquantes)\": f\"SELECT count(*) as nb_produits_sans_marque FROM {database_name}.brand_performance WHERE brand IS NULL\"\n",
    "}\n",
    "\n",
    "for title, sql in queries.items():\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "    spark.sql(sql).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Capstone_XhadeeZeydia",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
