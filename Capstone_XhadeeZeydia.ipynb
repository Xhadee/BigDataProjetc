{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7d1ec87-9fbf-4b73-8ad4-e95257b2a311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **PROJET DATA ENGINEERING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b9b1a4-8e35-4362-90e8-68cec4c89a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## INITIALISATION DE L'ENVIRONNEMENT ET CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2624bd49-b842-47e9-972b-bbd3df1c39b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "#  √âTAPE 1 : SCAFFOLDING (Architecture des Dossiers)\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA  = \"xhadeezeydia\"\n",
    "VOLUME  = \"capstoneipsl\"\n",
    "\n",
    "# Cr√©ation du volume si n√©cessaire\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n",
    "\n",
    "VOLUME_ROOT  = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n",
    "PROJECT_ROOT = f\"{VOLUME_ROOT}/ecommerce_project\"\n",
    "\n",
    "DIRECTORIES = [\n",
    "    \"data/bronze/main\", \"data/bronze/enrich\",\n",
    "    \"data/silver/main_clean\", \"data/silver/enrich_clean\", \"data/silver/joined\",\n",
    "    \"data/gold/marts\", \"data/gold/aggregates\", \"data/gold/exports\",\n",
    "    \"src/ingestion\", \"src/transforms\", \"src/quality\", \"src/utils\",\n",
    "    \"notebooks\", \"configs\",\n",
    "    \"reports/data_quality\", \"reports/benchmarks\"\n",
    "]\n",
    "\n",
    "# Cr√©ation physique des r√©pertoires sur le DBFS/Volumes\n",
    "for d in DIRECTORIES:\n",
    "    path = f\"{PROJECT_ROOT}/{d}\"\n",
    "    dbutils.fs.mkdirs(path)\n",
    "    print(f\"‚úì Created: {path}\")\n",
    "\n",
    "#  √âTAPE 2 : CR√âATION DU FICHIER CONFIG (SSOT ) \n",
    "config_data = {\n",
    "    \"project_metadata\": {\n",
    "        \"name\": \"E-Commerce Architecture Medallion\",\n",
    "        \"authors\": [\"Khady NDIAYE\",\"Seydou DIALLO\"],\n",
    "        \"filiere\": \"GIT3 - IPSL\"\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"project_root\": PROJECT_ROOT,\n",
    "        \"bronze_main\": f\"{PROJECT_ROOT}/data/bronze/main\",\n",
    "        \"silver_main\": f\"{PROJECT_ROOT}/data/silver/main_clean\",\n",
    "        \"gold_marts\": f\"{PROJECT_ROOT}/data/gold/marts\",\n",
    "        \"reports_quality\": f\"{PROJECT_ROOT}/reports/data_quality\"\n",
    "    },\n",
    "    \"business_rules\": {\n",
    "        \"default_margin_rate\": 0.10,\n",
    "        \"niche_margin_rate\": 0.15,\n",
    "        \"target_brands\": [\"runail\", \"grattol\", \"irisk\", \"uno\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sauvegarde physique du JSON dans le dossier /configs\n",
    "CONFIG_FILE_PATH = f\"{PROJECT_ROOT}/configs/pipeline_config.json\"\n",
    "\n",
    "with open(CONFIG_FILE_PATH, \"w\") as f:\n",
    "    json.dump(config_data, f, indent=4)\n",
    "\n",
    "print(f\"\\n‚úÖ Fichier de configuration g√©n√©r√© : {CONFIG_FILE_PATH}\")\n",
    "\n",
    "#  √âTAPE 3 : CHARGEMENT DE LA CONFIGURATION \n",
    "with open(CONFIG_FILE_PATH, \"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "# Variables globales utilisables dans tout le notebook\n",
    "PATH_BRONZE = cfg['paths']['bronze_main']\n",
    "PATH_SILVER = cfg['paths']['silver_main']\n",
    "MARGIN_RATE = cfg['business_rules']['default_margin_rate']\n",
    "\n",
    "print(f\"üöÄ Pipeline pr√™t. Pr√©nomm√© : {cfg['project_metadata']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2dc1b36-b585-45e3-85e4-e3f0fa6c6d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## L'Ingestion et l'Amplification(BRONZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5448c439-16e2-44c1-9867-bd0ee44ac218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Chargement (Source 1) - Utilisation du chemin dynamique\n",
    "df_raw = spark.read.csv(f\"{PROJECT_ROOT}/data/bronze/main/*.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2. Amplification Massive (x15 pour atteindre les ~9.30 Go)\n",
    "df_amplified = df_raw\n",
    "for _ in range(14):\n",
    "    df_amplified = df_amplified.unionAll(df_raw)\n",
    "\n",
    "# 3. √âcriture en PARQUET - Utilisation de PATH_BRONZE d√©fini dans la config\n",
    "# On ajoute un suffixe pour le fichier final\n",
    "bronze_output_path = f\"{PATH_BRONZE}/full_data.parquet\"\n",
    "df_amplified.write.mode(\"overwrite\").parquet(bronze_output_path)\n",
    "\n",
    "# 4. Source 2 (Enrichissement) - Utilisation des taux de marge de la config\n",
    "# On r√©cup√®re la marge par d√©faut depuis cfg\n",
    "default_margin = cfg['business_rules']['default_margin_rate']\n",
    "\n",
    "enrich_data = [\n",
    "    (\"electronics\", \"High-Tech\", 0.20), \n",
    "    (\"appliances\", \"Home\", 0.15), \n",
    "    (\"computers\", \"IT\", 0.18),\n",
    "    (\"others\", \"General\", default_margin) \n",
    "]\n",
    "\n",
    "df_enrich = spark.createDataFrame(enrich_data, [\"category_code_prefix\", \"category_department\", \"margin_rate\"])\n",
    "df_enrich.write.mode(\"overwrite\").parquet(f\"{PROJECT_ROOT}/data/bronze/enrich/static_ref.parquet\")\n",
    "\n",
    "# 5. Validation de la taille\n",
    "files = dbutils.fs.ls(bronze_output_path)\n",
    "size_gb = sum(f.size for f in files if f.name.endswith(\".parquet\")) / (1024**3)\n",
    "\n",
    "print(f\"‚úÖ BRONZE VALIDE : {size_gb:.2f} GB | Format: Parquet\")\n",
    "print(f\"üìç Stock√© dans : {bronze_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceddddf0-fb39-450d-9633-753115a46ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# On utilise le chemin d√©fini dans la config charg√©e pr√©c√©demment\n",
    "bronze_main_path = f\"{PATH_BRONZE}/full_data.parquet\"\n",
    "\n",
    "# On passe √† 20 fois la base (1 initiale + 19 unions) \n",
    "# Cela permet d'atteindre la cible symbolique des 9.30 Go\n",
    "df_amplified = df_raw\n",
    "for _ in range(19): \n",
    "    df_amplified = df_amplified.unionAll(df_raw)\n",
    "\n",
    "# √âcriture optimis√©e en Parquet\n",
    "df_amplified.write.mode(\"overwrite\").parquet(bronze_main_path)\n",
    "\n",
    "# V√©rification de la volum√©trie r√©elle sur le disque\n",
    "files = dbutils.fs.ls(bronze_main_path)\n",
    "size_gb = sum(f.size for f in files if f.name.endswith(\".parquet\")) / (1024**3)\n",
    "\n",
    "print(f\"üöÄ NOUVELLE TAILLE BRONZE : {size_gb:.2f} GB\")\n",
    "print(f\"üìä Nombre total de lignes : {df_amplified.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1008ba7a-5704-4c13-8798-4e7c061dccd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ZONE SILVER : RAFFINEMENT, AUDIT & GESTION DES REJETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fa2dcf3-38c2-4563-a8a3-3263e06cfa5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, trim, count, when, countDistinct, expr\n",
    "\n",
    "# 1. CHARGEMENT & PRUNING\n",
    "needed_columns = [\n",
    "    \"event_time\", \"event_type\", \"product_id\", \"category_id\", \n",
    "    \"category_code\", \"brand\", \"price\", \"user_id\", \"user_session\"\n",
    "]\n",
    "\n",
    "# CORRECTION : On pointe sur le dossier Parquet sp√©cifique g√©n√©r√© pr√©c√©demment\n",
    "# au lieu du dossier parent qui contient encore le fichier CSV.\n",
    "path_bronze_parquet = f\"{cfg['paths']['bronze_main']}/full_data.parquet\"\n",
    "df_bronze = spark.read.parquet(path_bronze_parquet).select(*needed_columns)\n",
    "\n",
    "# 2. GESTION DES REJETS (QUARANTAINE)\n",
    "valid_condition = (col(\"user_id\").isNotNull()) & (col(\"product_id\").isNotNull()) & (col(\"price\") > 0)\n",
    "\n",
    "df_valid = df_bronze.filter(valid_condition)\n",
    "df_quarantine = df_bronze.filter(~valid_condition)\n",
    "\n",
    "# 3. STANDARDISATION DES DONN√âES VALIDES\n",
    "df_silver = df_valid \\\n",
    "    .withColumn(\"event_type\", lower(trim(col(\"event_type\")))) \\\n",
    "    .withColumn(\"category_code\", lower(trim(col(\"category_code\")))) \\\n",
    "    .withColumn(\"brand\", lower(trim(col(\"brand\")))) \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"double\")) \\\n",
    "    .repartition(col(\"event_type\"))\n",
    "\n",
    "# 4. CALCUL DES INDICATEURS DE QUALIT√â \n",
    "\n",
    "total_rows = df_silver.count()\n",
    "\n",
    "# Si le dataset est vide, on √©vite la division par z√©ro\n",
    "if total_rows > 0:\n",
    "    quality_metrics = df_silver.select(\n",
    "        ((count(when(col(\"user_id\").isNotNull(), True)) / total_rows) * 100).alias(\"chk_1_user_complete\"),\n",
    "        ((count(when(col(\"price\") > 0, True)) / total_rows) * 100).alias(\"chk_2_price_pos\"),\n",
    "        ((count(when(col(\"event_type\").isNotNull(), True)) / total_rows) * 100).alias(\"chk_3_evt_present\"),\n",
    "        ((count(when(col(\"event_time\").isNotNull(), True)) / total_rows) * 100).alias(\"chk_4_date_valid\"),\n",
    "        ((count(when(col(\"brand\").isNotNull(), True)) / total_rows) * 100).alias(\"chk_5_brand_filled\"),\n",
    "        ((count(when(col(\"category_code\").isNotNull(), True)) / total_rows) * 100).alias(\"chk_6_cat_filled\"),\n",
    "        ((countDistinct(\"user_session\") / total_rows) * 100).alias(\"chk_7_unique_sessions\"),\n",
    "        ((count(when(col(\"user_session\").rlike(\"^[0-9a-fA-F-]+\"), True)) / total_rows) * 100).alias(\"chk_8_session_format\")\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Attention : df_silver est vide !\")\n",
    "\n",
    "# 5. SAUVEGARDES MULTI-NIVEAUX \n",
    "path_silver = cfg['paths']['silver_main']\n",
    "df_silver.write.mode(\"overwrite\").partitionBy(\"event_type\").parquet(path_silver)\n",
    "\n",
    "# B. Quarantaine\n",
    "path_quarantine = path_silver.replace(\"main_clean\", \"quarantine\")\n",
    "df_quarantine.write.mode(\"overwrite\").parquet(path_quarantine)\n",
    "\n",
    "# C. Rapport de Qualit√©\n",
    "path_report = f\"{cfg['paths']['reports_quality']}/silver_report.parquet\"\n",
    "quality_metrics.write.mode(\"overwrite\").parquet(path_report)\n",
    "\n",
    "# AFFICHAGE DES R√âSULTATS \n",
    "print(f\"‚úÖ PROCESSUS SILVER TERMIN√â\")\n",
    "print(f\"üìä Lignes Valides : {total_rows} | ‚ö†Ô∏è Lignes en Quarantaine : {df_quarantine.count()}\")\n",
    "quality_metrics.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "113ab253-37cb-4610-9478-6bc0bcc3e450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Zone GOLD:ANALYTICS & BUSINESS INTELLIGENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddc2a918-c468-4692-be68-f861b1ce80d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import broadcast, split, col, when, count, sum, round\n",
    "\n",
    "# 1. CHARGEMENT DES SOURCES \n",
    "# Utilisation des chemins issus de la configuration\n",
    "df_silver = spark.read.parquet(cfg['paths']['silver_main'])\n",
    "path_static = f\"{cfg['paths']['project_root']}/data/bronze/enrich/static_ref.parquet\"\n",
    "df_enrich = spark.read.parquet(path_static)\n",
    "\n",
    "# 2. PR√âPARATION ET JOINTURE BROADCAST \n",
    "# Extraction du pr√©fixe de cat√©gorie (ex: \"electronics.audio\" -> \"electronics\")\n",
    "df_silver_prep = df_silver.withColumn(\"cat_prefix\", split(col(\"category_code\"), r\"\\.\").getItem(0))\n",
    "\n",
    "# OPTIMISATION : Broadcast Join pour √©viter le Shuffle des 400M+ de lignes\n",
    "df_gold_base = df_silver_prep.join(\n",
    "    broadcast(df_enrich), \n",
    "    df_silver_prep.cat_prefix == df_enrich.category_code_prefix, \n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Calcul de la marge (Utilisation du taux par d√©faut du JSON si non trouv√©)\n",
    "DEFAULT_MARGIN_RATE = cfg['business_rules']['default_margin_rate']\n",
    "\n",
    "df_gold_base = df_gold_base.withColumn(\n",
    "    \"estimated_margin\", \n",
    "    when(col(\"margin_rate\").isNotNull(), col(\"price\") * col(\"margin_rate\"))\n",
    "    .otherwise(col(\"price\") * DEFAULT_MARGIN_RATE)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Enrichissement Gold termin√© avec Broadcast Join.\")\n",
    "\n",
    "# 3. G√âN√âRATION DES OUTPUTS ANALYTIQUES \n",
    "\n",
    "# A. Brand Performance (Revenue, Marge, AOV)\n",
    "gold_brand_perf = df_gold_base.filter(col(\"event_type\") == \"purchase\") \\\n",
    "    .groupBy(\"brand\") \\\n",
    "    .agg(\n",
    "        sum(\"price\").alias(\"total_revenue\"),\n",
    "        sum(\"estimated_margin\").alias(\"total_margin\"),\n",
    "        count(\"product_id\").alias(\"sales_count\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_order_value\", round(col(\"total_revenue\") / col(\"sales_count\"), 2)) \\\n",
    "    .withColumn(\"margin_percentage\", round((col(\"total_margin\") / col(\"total_revenue\")) * 100, 2)) \\\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "# B. Department Stats (Contribution au CA global)\n",
    "total_purchase_df = df_gold_base.filter(col(\"event_type\") == \"purchase\").agg(sum(\"price\").alias(\"global_rev\"))\n",
    "total_global_revenue = total_purchase_df.collect()[0][\"global_rev\"] or 1 # √âviter division par z√©ro\n",
    "\n",
    "gold_dept_stats = df_gold_base.filter(col(\"event_type\") == \"purchase\") \\\n",
    "    .groupBy(\"category_department\") \\\n",
    "    .agg(\n",
    "        count(\"event_type\").alias(\"total_sales\"),\n",
    "        sum(\"price\").alias(\"dept_revenue\")\n",
    "    ) \\\n",
    "    .withColumn(\"revenue_contribution_pct\", round((col(\"dept_revenue\") / total_global_revenue) * 100, 2)) \\\n",
    "    .dropna()\n",
    "\n",
    "# C. Funnel de Conversion (Pivot Purchase/Cart/View)\n",
    "gold_conversion_pivot = df_gold_base.groupBy(\"brand\") \\\n",
    "    .pivot(\"event_type\") \\\n",
    "    .agg(count(\"user_session\")) \\\n",
    "    .fillna(0) \\\n",
    "    .withColumn(\"conversion_rate_pct\", \n",
    "                round((col(\"purchase\") / col(\"view\")) * 100, 2))\n",
    "\n",
    "# 4. SAUVEGARDE FINALE DANS LA ZONE GOLD \n",
    "gold_path = cfg['paths']['gold_marts']\n",
    "\n",
    "gold_brand_perf.write.mode(\"overwrite\").parquet(f\"{gold_path}/brand_performance.parquet\")\n",
    "gold_dept_stats.write.mode(\"overwrite\").parquet(f\"{gold_path}/department_stats.parquet\")\n",
    "gold_conversion_pivot.write.mode(\"overwrite\").parquet(f\"{gold_path}/conversion_funnel.parquet\")\n",
    "\n",
    "print(f\"‚úÖ Tables Gold g√©n√©r√©es avec succ√®s dans : {gold_path}\")\n",
    "display(gold_brand_perf.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc5c6ce2-64a2-4ae2-8248-d22ceee1e5e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Performance : Benchmark \"Avant vs Apr√®s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "090c7438-dba3-4e4b-a0d6-c549eb14bc2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from pyspark.sql.functions import col, lit, round, when, concat\n",
    "\n",
    "# 1. Mesure brute (Bronze - Format CSV)\n",
    "start_csv = time.time()\n",
    "spark.read.csv(f\"{cfg['paths']['bronze_main']}/*.csv\", header=True).count()\n",
    "duration_csv = float(time.time() - start_csv)\n",
    "\n",
    "# 2. Mesure optimis√©e (Silver - Format Parquet Partitionn√©)\n",
    "# L'optimisation colonnaire de Parquet montre ici toute sa puissance\n",
    "start_pq = time.time()\n",
    "spark.read.parquet(cfg['paths']['silver_main']).count()\n",
    "duration_pq = float(time.time() - start_pq)\n",
    "\n",
    "# 3. Mesure de l'espace disque (Efficacit√© de la compression)\n",
    "def get_size(path):\n",
    "    try:\n",
    "        # On calcule la taille r√©elle occup√©e sur le Volume\n",
    "        total_size = sum(f.size for f in dbutils.fs.ls(path) if not f.name.startswith(\"_\"))\n",
    "        return float(total_size / (1024**3)) # Conversion en GB\n",
    "    except: \n",
    "        return 0.1 # Valeur par d√©faut en cas d'acc√®s restreint\n",
    "\n",
    "size_bronze = get_size(cfg['paths']['bronze_main'])\n",
    "size_silver = get_size(cfg['paths']['silver_main'])\n",
    "\n",
    "# 4. Construction du rapport de performance\n",
    "raw_data = [\n",
    "    (\"Temps de lecture (sec)\", duration_csv, duration_pq),\n",
    "    (\"Espace disque (GB)\", size_bronze, size_silver)\n",
    "]\n",
    "\n",
    "df_bench = spark.createDataFrame(raw_data, [\"Metrique\", \"Brut_CSV\", \"Optimise_Parquet\"])\n",
    "\n",
    "# 5. Calcul dynamique des gains\n",
    "df_final = df_bench.withColumn(\n",
    "    \"Gain\",\n",
    "    when(col(\"Metrique\").contains(\"Temps\"), \n",
    "         concat(round(col(\"Brut_CSV\") / col(\"Optimise_Parquet\"), 1), lit(\"x plus rapide\")))\n",
    "    .otherwise(\n",
    "         concat(round((1 - (col(\"Optimise_Parquet\") / col(\"Brut_CSV\"))) * 100, 1), lit(\"% de reduction\")))\n",
    ")\n",
    "\n",
    "print(\"üöÄ R√âSULTATS DU BENCHMARK DE PERFORMANCE :\")\n",
    "df_final.show(truncate=False)\n",
    "\n",
    "# 6. Sauvegarde du rapport pour le dossier 'reports'\n",
    "report_path = f\"{cfg['paths']['project_root']}/reports/benchmarks/performance_final.parquet\"\n",
    "df_final.write.mode(\"overwrite\").parquet(report_path)\n",
    "\n",
    "print(f\"‚úÖ Rapport de performance archiv√© dans : {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "752424e6-e424-4540-a450-1a5e5335fff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Export  & Requ√™tes SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b4c9b71-aff6-490d-a4ef-4a1b9edc4820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "#  1. CHARGEMENT DES TABLES GOLD DEPUIS LE VOLUME \n",
    "gold_path = cfg['paths']['gold_marts']\n",
    "\n",
    "df_brand  = spark.read.parquet(f\"{gold_path}/brand_performance.parquet\")\n",
    "df_dept   = spark.read.parquet(f\"{gold_path}/department_stats.parquet\")\n",
    "df_funnel = spark.read.parquet(f\"{gold_path}/conversion_funnel.parquet\")\n",
    "\n",
    "#  2. EXPORT VERS LE METASTORE (BASE SQL INTERNE) \n",
    "# Cette √©tape permet d'utiliser le langage SQL sur nos fichiers Parquet\n",
    "database_name = \"ecommerce_analytics_db\"\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "\n",
    "df_brand.write.mode(\"overwrite\").saveAsTable(f\"{database_name}.brand_performance\")\n",
    "df_dept.write.mode(\"overwrite\").saveAsTable(f\"{database_name}.department_stats\")\n",
    "df_funnel.write.mode(\"overwrite\").saveAsTable(f\"{database_name}.conversion_funnel\")\n",
    "\n",
    "print(f\"‚úÖ Base '{database_name}' synchronis√©e. (3 tables expos√©es)\")\n",
    "\n",
    "# 3. NOTE TECHNIQUE : EXPORT EXTERNE (POSTGRESQL/JDBC)\n",
    "#  Voici la m√©thode pour exporter vers un serveur externe \n",
    "# conforme aux exigences de connectivit√© JDBC :\n",
    "\"\"\"\n",
    "df_brand.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://<host>:<port>/ecommerce_db\") \\\n",
    "    .option(\"dbtable\", \"final_brand_performance\") \\\n",
    "    .option(\"user\", \"admin\") \\\n",
    "    .option(\"password\", \"password123\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\"\"\"\n",
    "\n",
    "# 4. EX√âCUTION DU RAPPORT ANALYTIQUE SQL\n",
    "print(\"\\nüìä G√âN√âRATION DU RAPPORT DE D√âCISION M√âTIER :\")\n",
    "\n",
    "# Requ√™te A : Top 5 Profitabilit√© (CA significatif > 100k)\n",
    "query_top_marge = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        brand, \n",
    "        round(total_revenue, 2) as CA, \n",
    "        round(total_margin, 2) as Marge_Net,\n",
    "        margin_percentage as Rentabilite_Pct\n",
    "    FROM {database_name}.brand_performance\n",
    "    WHERE brand IS NOT NULL AND total_revenue > 100000\n",
    "    ORDER BY total_margin DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "# Requ√™te B : Taux de Conversion (Efficacit√© Marketing)\n",
    "query_conversion = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        brand, \n",
    "        view as Vues, \n",
    "        purchase as Ventes, \n",
    "        round(conversion_rate_pct, 2) as Taux_Conversion_Pct\n",
    "    FROM {database_name}.conversion_funnel\n",
    "    WHERE view > 1000\n",
    "    ORDER BY conversion_rate_pct DESC \n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "# Requ√™te C : Analyse des D√©partements (Poids Relatif)\n",
    "query_dept = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        category_department as Departement, \n",
    "        total_sales as Nb_Ventes, \n",
    "        revenue_contribution_pct as Contribution_CA_Pct\n",
    "    FROM {database_name}.department_stats\n",
    "    ORDER BY Contribution_CA_Pct DESC\n",
    "\"\"\")\n",
    "\n",
    "# --- AFFICHAGE DES R√âSULTATS ---\n",
    "print(\"\\n--- 1. TOP 5 MARQUES PAR PROFITABILIT√â ---\")\n",
    "query_top_marge.show()\n",
    "\n",
    "print(\"\\n--- 2. TOP 5 MARQUES PAR CONVERSION (Marketing) ---\")\n",
    "query_conversion.show()\n",
    "\n",
    "print(\"\\n--- 3. PERFORMANCE PAR D√âPARTEMENT ---\")\n",
    "query_dept.show()\n",
    "\n",
    "# --- 5. EXPORT DU RAPPORT FINAL (Pour archivage) ---\n",
    "report_export_path = f\"{cfg['paths']['project_root']}/reports/exports_sql/final_summary\"\n",
    "query_top_marge.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(report_export_path)\n",
    "\n",
    "print(f\"‚úÖ Rapport final archiv√© en CSV dans : {report_export_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Capstone_XhadeeZeydia",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
